{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGt10T5969EL"
      },
      "source": [
        "\n",
        "# Importacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvmCfbgV7aia"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvXYIVH4V6dW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from entrenamiento import genera_dataframe, almacena_bn,\\\n",
        "    genera_dataframe_bn, borrado_entrenamiento_test,\\\n",
        "    plot_prediction, convertir_jpeg, preprocesado\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score,\\\n",
        "    precision_score, roc_auc_score \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Activation, Dropout,\\\n",
        "    MaxPooling2D\n",
        "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
        "import seaborn as sns\n",
        "import tensorflow_addons as tfa\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variables"
      ],
      "metadata": {
        "id": "C9G_OXJqFSMV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kodeeaINW78v"
      },
      "outputs": [],
      "source": [
        "# Ruta\n",
        "path_incendios = '/content/gdrive/My Drive/incendios_satelite/Incendios masivos/'\n",
        "path_no_incendios = '/content/gdrive/My Drive/incendios_satelite/Control/'\n",
        "path_train_incendio = '/content/gdrive/My Drive/incendios_satelite/Entrenamiento_incendio/'\n",
        "path_train_no_incendio = '/content/gdrive/My Drive/incendios_satelite/Entrenamiento_no_incendio/'\n",
        "path_test_incendio = '/content/gdrive/My Drive/incendios_satelite/Validacion_incendio/'\n",
        "path_test_no_incendio = '/content/gdrive/My Drive/incendios_satelite/Validacion_no_incendio/'\n",
        "path_incendios = '/content/gdrive/My Drive/incendios_satelite/Incendios masivos/'\n",
        "path_no_incendios = '/content/gdrive/My Drive/incendios_satelite/Control/'\n",
        "var_real ='/content/gdrive/My Drive/incendios_satelite/1reales/'\n",
        "var_lst ='/content/gdrive/My Drive/incendios_satelite/2lst/'\n",
        "var_nvdi ='/content/gdrive/My Drive/incendios_satelite/3nvdi/'\n",
        "var_poblacion ='/content/gdrive/My Drive/incendios_satelite/4poblacion/'\n",
        "var_viento_norte ='/content/gdrive/My Drive/incendios_satelite/5viento_norte/'\n",
        "var_viento_este ='/content/gdrive/My Drive/incendios_satelite/6viento_este/'\n",
        "ruta_training = 'training_data/'\n",
        "ruta_test = 'validation_data/'\n",
        "ruta_class_0 = 'class_0/'\n",
        "ruta_class_1 = 'class_1/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUOY33XX7HON"
      },
      "source": [
        "# Genero lista de imágenes y de control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaeSqAEP7GjP"
      },
      "outputs": [],
      "source": [
        "lst_incendios_train = []\n",
        "lst_incendios_test = []\n",
        "\n",
        "# Listado de números\n",
        "for file in os.listdir(path_incendios):\n",
        "  if \"LST_Day\" in file:\n",
        "    id_imagen = int(file.split(\"L\")[0])\n",
        "    if random.random() < 0.8:\n",
        "      lst_incendios_train.append(id_imagen)\n",
        "    else:\n",
        "      lst_incendios_test.append(id_imagen)\n",
        "\n",
        "for file in os.listdir(path_no_incendios):\n",
        "  if \"LST_Day\" in file:\n",
        "    id_imagen = int(file.split(\"L\")[0])\n",
        "    if random.random() < 0.8:\n",
        "      lst_incendios_train.append(id_imagen)\n",
        "    else:\n",
        "      lst_incendios_test.append(id_imagen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvas0rE6-Deq"
      },
      "outputs": [],
      "source": [
        "# Reparto en carpetas\n",
        "for file in os.listdir(path_incendios):\n",
        "  if \"LST_Day\" in file:\n",
        "    id_imagen = int(file.split(\"L\")[0])\n",
        "  elif \"ND\" in file:\n",
        "    id_imagen = int(file.split(\"N\")[0])\n",
        "\n",
        "  if id_imagen in lst_incendios_train:\n",
        "    shutil.copy(path_incendios + file, path_train_incendio)\n",
        "  elif id_imagen in lst_incendios_test:\n",
        "    shutil.copy(path_incendios + file, path_test_incendio)\n",
        "\n",
        "for file in os.listdir(path_no_incendios):\n",
        "  if \"LST_Day\" in file:\n",
        "    id_imagen = int(file.split(\"L\")[0])\n",
        "  elif \"ND\" in file:\n",
        "    id_imagen = int(file.split(\"N\")[0])\n",
        "\n",
        "  if id_imagen in lst_incendios_train:\n",
        "    shutil.copy(path_no_incendios + file, path_train_no_incendio)\n",
        "  elif id_imagen in lst_incendios_test:\n",
        "    shutil.copy(path_no_incendios + file, path_test_no_incendio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-aTDbETsIS-"
      },
      "source": [
        "# Preprocesado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CO0Cuc8WgN7"
      },
      "outputs": [],
      "source": [
        "# Convertir a JPEG\n",
        "convertir_jpeg(var_lst + ruta_training + ruta_class_1)\n",
        "convertir_jpeg(var_lst + ruta_training + ruta_class_0)\n",
        "convertir_jpeg(var_lst + ruta_test + ruta_class_1)\n",
        "convertir_jpeg(var_lst + ruta_test + ruta_class_0)\n",
        "convertir_jpeg(var_nvdi + ruta_training + ruta_class_1)\n",
        "convertir_jpeg(var_nvdi + ruta_training + ruta_class_0)\n",
        "convertir_jpeg(var_nvdi + ruta_test + ruta_class_1)\n",
        "convertir_jpeg(var_nvdi + ruta_test + ruta_class_0)\n",
        "convertir_jpeg(var_poblacion + ruta_training + ruta_class_1)\n",
        "convertir_jpeg(var_poblacion + ruta_training + ruta_class_0)\n",
        "convertir_jpeg(var_poblacion + ruta_test + ruta_class_1)\n",
        "convertir_jpeg(var_poblacion + ruta_test + ruta_class_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2VB2rAByzYr"
      },
      "source": [
        "# Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuJWpT6qxppB"
      },
      "source": [
        "## Red Convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlGUK0DtIXbl"
      },
      "source": [
        "# Temperatura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5fJpnYTrMj"
      },
      "source": [
        "## Preparación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "posmIAKZsITK"
      },
      "outputs": [],
      "source": [
        "# DATOS DL\n",
        "# Generamos el dataframe de temperatura\n",
        "temp_train_0 = '/content/gdrive/My Drive/incendios_satelite/2lst/training_data/class_0/'\n",
        "temp_train_1 = '/content/gdrive/My Drive/incendios_satelite/2lst/training_data/class_1/'\n",
        "temp_test_0 = '/content/gdrive/My Drive/incendios_satelite/2lst/validation_data/class_0/'\n",
        "temp_test_1 = '/content/gdrive/My Drive/incendios_satelite/2lst/validation_data/class_1/'\n",
        "\n",
        "inicio = time.time()\n",
        "df_temp_train, lst_features_x, df_id = genera_dataframe(ruta_incendio = temp_train_1,\n",
        "                                      ruta_no_incendio = temp_train_0,\n",
        "                                      radar= \"LST_Day\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "df_temp_test, lst_features_x, df_id = genera_dataframe(ruta_incendio = temp_test_1,\n",
        "                                      ruta_no_incendio = temp_test_0,\n",
        "                                      radar= \"LST_Day\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHCvPxHXUtYK"
      },
      "outputs": [],
      "source": [
        "# DATOS DL\n",
        "kernel_size = [2, 3, 5, 8]\n",
        "lst_lr = [0.001, 0.01, 0.1]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lado = 23 # Ancho y largo del lado de la imagen\n",
        "lst_neuronas = [32, 64, 128, 256]\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow\n",
        "\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/2lst/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/2lst/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = validation_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC2p0FNcTy0m"
      },
      "source": [
        "## Modelos ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0jDNWdMsITN"
      },
      "outputs": [],
      "source": [
        "# Generamos procesos previos:\n",
        "X_train, X_test, y_train, y_test = preprocesado(df_train = df_temp_train, df_test = df_temp_test)\n",
        "\n",
        "# REGRESIÓN LOGÍSTICA\n",
        "# Hiperparámetros\n",
        "C = [0.9, 0.8, 0.7] # Intensidad de la regularización\n",
        "regularization_methods = [\"l1\", \"l2\"] # Métodos de regularización\n",
        "max_iterations = [100, 1000, 10000, 100000] # Números máximos de iteraciones\n",
        "\n",
        "temperatura_result_logistica = regresion_logistica(X_train = X_train,\n",
        "                                              X_test = X_test,\n",
        "                                              y_train = y_train,\n",
        "                                              y_test = y_test,\n",
        "                                              C = C, \n",
        "                                regularization_methods = regularization_methods,\n",
        "                                max_iterations = max_iterations)\n",
        "\n",
        "# XGBOOST\n",
        "# Hiperparámetros\n",
        "learning_rate = [0.001, 0.01, 0.1] # Tasa de aprendizaje\n",
        "max_depth = [5, 10, 20] # Profundidaz máxima\n",
        "n_estimators = [10, 50, 100] # Número de árboles\n",
        "temperatura_result_xgboost = func_xgboost(X_train = X_train,\n",
        "                                      X_test = X_test,\n",
        "                                      y_train = y_train,\n",
        "                                      y_test = y_test,\n",
        "                                      learning_rate = learning_rate,\n",
        "                                      max_depth = max_depth,\n",
        "                                      n_estimators = n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei6XIz4QsITO"
      },
      "outputs": [],
      "source": [
        "temp_best_result_log = max(\n",
        "    temperatura_result_logistica, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzV4iz60sITT"
      },
      "outputs": [],
      "source": [
        "temp_best_result_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4ndKKcLsITU"
      },
      "outputs": [],
      "source": [
        "temp_best_result_xgboost = max(\n",
        "    temperatura_result_xgboost, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOVzvvqUsITV"
      },
      "outputs": [],
      "source": [
        "temp_best_result_xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg9x6poHT3ld"
      },
      "source": [
        "## Modelos DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm9OscbI1udF"
      },
      "outputs": [],
      "source": [
        "resultados_red_densa_temp = red_densa(train = train_ds,\n",
        "                                    test = validation_ds,\n",
        "                                    lst_neuronas = lst_neuronas, \n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    input_shape= lado*lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEsy9Fx61-rn"
      },
      "outputs": [],
      "source": [
        "resultados_conv_temp = red_convolucional(train = train_conv,\n",
        "                                    test = validation_conv,\n",
        "                                    kernel_size = kernel_size,\n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    lado = lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrxjI3BU2I1J"
      },
      "outputs": [],
      "source": [
        "temp_best_result_rd = max(\n",
        "    resultados_red_densa_temp, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejnTcXyyBf3Q"
      },
      "outputs": [],
      "source": [
        "temp_best_result_rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQbAZBHL2JCp"
      },
      "outputs": [],
      "source": [
        "temp_best_result_conv = max(\n",
        "    resultados_conv_temp, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Vd9i_y0WbN"
      },
      "outputs": [],
      "source": [
        "temp_best_result_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgsB-yTL2mlU"
      },
      "outputs": [],
      "source": [
        "temp_best_result_rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzO0Zan02mu7"
      },
      "outputs": [],
      "source": [
        "temp_best_result_conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBfBi60GkfpV"
      },
      "source": [
        "# Humedad relativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib_VD3UsT7KX"
      },
      "source": [
        "## Preparación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "plr9I0SokiwT"
      },
      "outputs": [],
      "source": [
        "# DATOS ML\n",
        "# Generamos el dataframe de humedad relativa\n",
        "hum_train_0 = '/content/gdrive/My Drive/incendios_satelite/3nvdi/training_data/class_0/'\n",
        "hum_train_1 = '/content/gdrive/My Drive/incendios_satelite/3nvdi/training_data/class_1/'\n",
        "hum_test_0 = '/content/gdrive/My Drive/incendios_satelite/3nvdi/validation_data/class_0/'\n",
        "hum_test_1 = '/content/gdrive/My Drive/incendios_satelite/3nvdi/validation_data/class_1/'\n",
        "\n",
        "inicio = time.time()\n",
        "df_hum_train, lst_features_x, df_id = genera_dataframe(ruta_incendio = hum_train_1,\n",
        "                                      ruta_no_incendio = hum_train_0,\n",
        "                                      radar= \"NDVI\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "df_hum_test, lst_features_x, df_id = genera_dataframe(ruta_incendio = hum_test_1,\n",
        "                                      ruta_no_incendio = hum_test_0,\n",
        "                                      radar= \"NDVI\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dk010f5yNTB"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow\n",
        "# DATOS DL\n",
        "kernel_size = [2, 3, 5, 8]\n",
        "lst_lr = [0.001, 0.01, 0.1]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lado = 23 # Ancho y largo del lado de la imagen\n",
        "lst_neuronas = [32, 64, 128, 256]\n",
        "\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/3nvdi/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/3nvdi/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = validation_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLmzU-cAT_pV"
      },
      "source": [
        "## Modelos ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qpOvcuWYsITe"
      },
      "outputs": [],
      "source": [
        "inicio = time.time()\n",
        "# Generamos procesos previos:\n",
        "X_train, X_test, y_train, y_test = preprocesado(df_train = df_hum_train, df_test = df_hum_test)\n",
        "\n",
        "# REGRESIÓN LOGÍSTICA\n",
        "# Hiperparámetros\n",
        "C = [0.9, 0.8, 0.7] # Lista de tasas de aprendizaje\n",
        "regularization_methods = [\"l1\", \"l2\"] # Métodos de regularización\n",
        "max_iterations = [100, 1000, 10000, 100000] # Números máximos de iteraciones\n",
        "\n",
        "humedad_result_logistica = regresion_logistica(X_train = X_train,\n",
        "                                              X_test = X_test,\n",
        "                                              y_train = y_train,\n",
        "                                              y_test = y_test,\n",
        "                                              C = C, \n",
        "                                regularization_methods = regularization_methods,\n",
        "                                max_iterations = max_iterations)\n",
        "\n",
        "# XGBOOST\n",
        "# Hiperparámetros\n",
        "learning_rate = [0.001, 0.01, 0.1] # Tasa de aprendizaje\n",
        "max_depth = [5, 10, 20] # Profundidaz máxima\n",
        "n_estimators = [10, 50, 100, 200, 300] # Número de árboles\n",
        "humedad_result_xgboost = func_xgboost(X_train = X_train,\n",
        "                                      X_test = X_test,\n",
        "                                      y_train = y_train,\n",
        "                                      y_test = y_test,\n",
        "                                      learning_rate = learning_rate,\n",
        "                                      max_depth = max_depth,\n",
        "                                      n_estimators = n_estimators)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16hOfU9ysITf"
      },
      "outputs": [],
      "source": [
        "humedad_best_result_log = max(\n",
        "    humedad_result_logistica, key=lambda x: x['f1_score'])\n",
        "humedad_best_result_xgboost = max(\n",
        "    humedad_result_xgboost, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVFA5-ITsITg"
      },
      "outputs": [],
      "source": [
        "humedad_best_result_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1NuUaI-sITj"
      },
      "outputs": [],
      "source": [
        "humedad_best_result_xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of6350-PUFPA"
      },
      "source": [
        "## Modelos DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a-4O3Q3zsDP"
      },
      "outputs": [],
      "source": [
        "lst_neuronas = [128]\n",
        "resultados_red_densa = red_densa(train = train_ds,\n",
        "                                    test = validation_ds,\n",
        "                                    lst_neuronas = lst_neuronas, \n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    input_shape= lado*lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9vCvxvEztDK"
      },
      "outputs": [],
      "source": [
        "resultados_conv = red_convolucional(train = train_conv,\n",
        "                                    test = validation_conv,\n",
        "                                    kernel_size = kernel_size,\n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    lado = lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsezswufUeoi"
      },
      "outputs": [],
      "source": [
        "hum_best_result_rd = max(\n",
        "    resultados_red_densa, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imh9n0L-Ueoj"
      },
      "outputs": [],
      "source": [
        "hum_best_result_conv = max(\n",
        "    resultados_conv, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSg4PGV_Ueoj"
      },
      "outputs": [],
      "source": [
        "hum_best_result_rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijvW7YYvUeoj"
      },
      "outputs": [],
      "source": [
        "hum_best_result_conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeXm0SnlIrb"
      },
      "source": [
        "# Población"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNxHzHgXUlr_"
      },
      "source": [
        "## Preparación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zPJW-NGsITl"
      },
      "outputs": [],
      "source": [
        "# Generamos el dataframe de temperatura\n",
        "inicio = time.time()\n",
        "pob_train_0 = '/content/gdrive/My Drive/incendios_satelite/4poblacion/training_data/class_0/'\n",
        "pob_train_1 = '/content/gdrive/My Drive/incendios_satelite/4poblacion/training_data/class_1/'\n",
        "pob_test_0 = '/content/gdrive/My Drive/incendios_satelite/4poblacion/validation_data/class_0/'\n",
        "pob_test_1 = '/content/gdrive/My Drive/incendios_satelite/4poblacion/validation_data/class_1/'\n",
        "\n",
        "df_pob_train, lst_features_x, df_id = genera_dataframe(ruta_incendio = pob_train_1,\n",
        "                                      ruta_no_incendio = pob_train_0,\n",
        "                                      radar= \"smod\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "df_pob_test, lst_features_x, df_id = genera_dataframe(ruta_incendio = pob_test_1,\n",
        "                                      ruta_no_incendio = pob_test_0,\n",
        "                                      radar= \"smod\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcEQuPP-XbvH"
      },
      "outputs": [],
      "source": [
        "# DATOS DL\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "kernel_size = [2, 3, 5, 8]\n",
        "lst_lr = [0.001, 0.01, 0.1]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lado = 23 # Ancho y largo del lado de la imagen\n",
        "lst_neuronas = [32, 64, 128, 256]\n",
        "\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/4poblacion/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/4poblacion/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = validation_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuvxNZKOXh6R"
      },
      "source": [
        "## Modelos ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uPmit-dQsITn"
      },
      "outputs": [],
      "source": [
        "inicio = time.time()\n",
        "# Generamos procesos previos:\n",
        "X_train, X_test, y_train, y_test = preprocesado(df_train = df_pob_train, df_test = df_pob_test)\n",
        "\n",
        "# REGRESIÓN LOGÍSTICA\n",
        "# Hiperparámetros\n",
        "C = [0.9, 0.8, 0.7] # Lista de tasas de aprendizaje\n",
        "regularization_methods = [\"l1\", \"l2\"] # Métodos de regularización\n",
        "max_iterations = [100, 1000, 10000, 10000] # Números máximos de iteraciones\n",
        "\n",
        "poblacion_result_logistica = regresion_logistica(X_train = X_train,\n",
        "                                              X_test = X_test,\n",
        "                                              y_train = y_train,\n",
        "                                              y_test = y_test,\n",
        "                                              C = C, \n",
        "                                regularization_methods = regularization_methods,\n",
        "                                max_iterations = max_iterations)\n",
        "\n",
        "# XGBOOST\n",
        "# Hiperparámetros\n",
        "learning_rate = [0.001, 0.01, 0.1] # Tasa de aprendizaje\n",
        "max_depth = [5, 10, 20] # Profundidaz máxima\n",
        "n_estimators = [10, 50, 100, 200, 300, 400] # Número de árboles\n",
        "poblacion_result_xgboost = func_xgboost(X_train = X_train,\n",
        "                                      X_test = X_test,\n",
        "                                      y_train = y_train,\n",
        "                                      y_test = y_test,\n",
        "                                      learning_rate = learning_rate,\n",
        "                                      max_depth = max_depth,\n",
        "                                      n_estimators = n_estimators)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fBvQqX4bsITo"
      },
      "outputs": [],
      "source": [
        "poblacion_best_result_log = max(\n",
        "    poblacion_result_logistica, key=lambda x: x['f1_score'])\n",
        "poblacion_best_result_xgboost = max(\n",
        "    poblacion_result_xgboost, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qi8yYUFZsITp"
      },
      "outputs": [],
      "source": [
        "poblacion_best_result_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DEAyZB-sITp"
      },
      "outputs": [],
      "source": [
        "poblacion_best_result_xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0me69jRUXYqA"
      },
      "source": [
        "## Modelos DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPQTKVXeXsQw"
      },
      "outputs": [],
      "source": [
        "resultados_red_densa = red_densa(train = train_ds,\n",
        "                                    test = validation_ds,\n",
        "                                    lst_neuronas = lst_neuronas, \n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    input_shape= lado*lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0k78re6XsQw"
      },
      "outputs": [],
      "source": [
        "resultados_conv = red_convolucional(train = train_conv,\n",
        "                                    test = validation_conv,\n",
        "                                    kernel_size = kernel_size,\n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    lado = lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgYUr2mOXsQx"
      },
      "outputs": [],
      "source": [
        "pob_best_result_rd = max(\n",
        "    resultados_red_densa, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtaV2L5yXsQx"
      },
      "outputs": [],
      "source": [
        "pob_best_result_conv = max(\n",
        "    resultados_conv, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rniZemK4XsQx"
      },
      "outputs": [],
      "source": [
        "pob_best_result_rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtJE7rj7XsQx"
      },
      "outputs": [],
      "source": [
        "pob_best_result_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap7hCwtCdlAK"
      },
      "outputs": [],
      "source": [
        "resultados_red_densa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMWFKKtoaheo"
      },
      "source": [
        "# Velocidad del viento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcNhnDdFyWk6"
      },
      "source": [
        "## Análisis de distribución"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd-eMO_myaCt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "\n",
        "def promedio(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "inicio = time.time()\n",
        "lst_archivos_incendios = []\n",
        "lst_archivos_no_incendios = []\n",
        "radar = \"wind\"\n",
        "reshape_x = 23\n",
        "reshape_y = 23\n",
        "\n",
        "# Listado de incendios\n",
        "lista_ficheros_incendios = []\n",
        "n_varibles_x =reshape_x * reshape_y\n",
        "print(\"Creando lista de incendios a leer...\")\n",
        "for file in os.listdir(path_incendios):\n",
        "      if radar in file:\n",
        "            lst_archivos_incendios.append(file)\n",
        "\n",
        "for file in os.listdir(path_no_incendios):\n",
        "      if radar in file:\n",
        "            lst_archivos_no_incendios.append(file)  \n",
        "\n",
        "lista_viento = []\n",
        "\n",
        "n_varibles_x = reshape_x * reshape_y\n",
        "\n",
        "for i in lst_archivos_incendios:\n",
        "    if \"north\" in i:\n",
        "        token = i.split(\"wind\")[0]\n",
        "    \n",
        "        for j in lst_archivos_incendios:\n",
        "            if j.split(\"wind\")[0] == token and \"east\" in j:\n",
        "                lst_item = []\n",
        "                imagen1 = cv2.imread(path_incendios + i\n",
        "                                )[0:reshape_x, 0:reshape_y, :]\n",
        "                image_grey1 = np.array(cv2.cvtColor(imagen1, cv2.COLOR_BGR2GRAY))\n",
        "                image_grey_reshape_1 = image_grey1.reshape(n_varibles_x)\n",
        "                lst_grey_reshape_1 = image_grey_reshape_1.tolist()\n",
        "                viento_u = promedio(lst_grey_reshape_1)\n",
        "    \n",
        "                imagen2 = cv2.imread(path_incendios + j\n",
        "                                )[0:reshape_x, 0:reshape_y, :]\n",
        "                image_grey2 = np.array(cv2.cvtColor(imagen2, cv2.COLOR_BGR2GRAY))\n",
        "                image_grey_reshape_2 = image_grey2.reshape(n_varibles_x)\n",
        "                lst_grey_reshape_2 = image_grey_reshape_2.tolist()\n",
        "                viento_v = promedio(lst_grey_reshape_2)\n",
        "            \n",
        "                viento = math.sqrt(viento_u**2 + viento_v**2)\n",
        "                id = int(i.split('w')[0])\n",
        "                lst_item.append(viento)\n",
        "                lst_item.append(1)\n",
        "                lst_item.append(id)\n",
        "                lista_viento.append(lst_item)\n",
        "            \n",
        "for i in lst_archivos_no_incendios:\n",
        "    if \"north\" in i:\n",
        "        token = i.split(\"wind\")[0]\n",
        "    \n",
        "        for j in lst_archivos_no_incendios:\n",
        "            if j.split(\"wind\")[0] == token and \"east\" in j:\n",
        "                lst_item = []\n",
        "                imagen1 = cv2.imread(path_no_incendios + i\n",
        "                                )[0:reshape_x, 0:reshape_y, :]\n",
        "                image_grey1 = np.array(cv2.cvtColor(imagen1, cv2.COLOR_BGR2GRAY))\n",
        "                image_grey_reshape_1 = image_grey1.reshape(n_varibles_x)\n",
        "                lst_grey_reshape_1 = image_grey_reshape_1.tolist()\n",
        "                viento_u = promedio(lst_grey_reshape_1)\n",
        "            \n",
        "                imagen2 = cv2.imread(path_no_incendios + j\n",
        "                                )[0:reshape_x, 0:reshape_y, :]\n",
        "                image_grey2 = np.array(cv2.cvtColor(imagen2, cv2.COLOR_BGR2GRAY))\n",
        "                image_grey_reshape_2 = image_grey2.reshape(n_varibles_x)\n",
        "                lst_grey_reshape_2 = image_grey_reshape_2.tolist()\n",
        "                viento_v = promedio(lst_grey_reshape_2)\n",
        "            \n",
        "                viento = math.sqrt(viento_u**2 + viento_v**2)\n",
        "                id = int(i.split('w')[0])\n",
        "                         \n",
        "                lst_item.append(viento)\n",
        "                lst_item.append(0)\n",
        "                lst_item.append(id)\n",
        "                lista_viento.append(lst_item)\n",
        "\n",
        "\n",
        "columns = [\"col_x\",\"target\",\"id\"]\n",
        "  # Genero el dataframe en sí\n",
        "df_viento = pd.DataFrame(data = lista_viento, columns =columns )\n",
        "\n",
        "# Barajo el dataframe\n",
        "df_viento=df_viento.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))\n",
        "df_viento.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wGsHhPA4Uhz"
      },
      "outputs": [],
      "source": [
        "# Genero gráfico de cajas y bigotes\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Supongamos que tenemos los siguientes datos:\n",
        "velocidad_viento_0 = df_viento[df_viento[\"target\"] == 0][\"col_x\"].to_list()\n",
        "velocidad_viento_1 = df_viento[df_viento[\"target\"] == 1][\"col_x\"].to_list()\n",
        "\n",
        "# Creamos el gráfico de barras y bigotes\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Creamos los bigotes\n",
        "bigotes = np.array([[np.min(velocidad_viento_0), np.max(velocidad_viento_0)], \n",
        "                    [np.min(velocidad_viento_1), np.max(velocidad_viento_1)]])\n",
        "\n",
        "# Creamos las barras\n",
        "barras = np.array([np.mean(velocidad_viento_0), np.mean(velocidad_viento_1)])\n",
        "\n",
        "# Dibujamos el gráfico\n",
        "plt.boxplot([velocidad_viento_0, velocidad_viento_1])\n",
        "\n",
        "# Añadimos etiquetas ejes y título\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Velocidad del viento')\n",
        "plt.title('Boxplot velocidad del viento')\n",
        "plt.xticks([1, 2], [0, 1])\n",
        "\n",
        "# Mostramos el gráfico\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-1DHNM7tSS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creamos el primer histograma\n",
        "plt.hist(velocidad_viento_0, label='target = 0')\n",
        "\n",
        "# Creamos el segundo histograma\n",
        "plt.hist(velocidad_viento_1, label='target = 1')\n",
        "\n",
        "# Añadimos etiquetas ejes y título\n",
        "plt.xlabel('Velocidad del viento')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Distribución velocidad del viento')\n",
        "\n",
        "# Añadimos la leyenda\n",
        "plt.legend()\n",
        "\n",
        "# Mostramos el gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBnGyRn7TMld"
      },
      "outputs": [],
      "source": [
        "# Grabo el resultado\n",
        "df_viento.to_csv('/content/gdrive/My Drive/incendios_satelite/df_viento.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilbd82utzw4K"
      },
      "source": [
        "# Imagen real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiAjsFLg2TMd"
      },
      "source": [
        "## Preparación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTXGZ0DTzyNE"
      },
      "outputs": [],
      "source": [
        "# Generamos el dataframe de las imágenes reales\n",
        "real_train_0 = '/content/gdrive/My Drive/incendios_satelite/1reales/training_data/class_0/'\n",
        "real_train_1 = '/content/gdrive/My Drive/incendios_satelite/1reales/training_data/class_1/'\n",
        "real_test_0 = '/content/gdrive/My Drive/incendios_satelite/1reales/validation_data/class_0/'\n",
        "real_test_1 = '/content/gdrive/My Drive/incendios_satelite/1reales/validation_data/class_1/'\n",
        "\n",
        "inicio = time.time()\n",
        "df_real_train, lst_features_x, df_id = genera_dataframe(ruta_incendio = real_train_1,\n",
        "                                      ruta_no_incendio = real_train_0,\n",
        "                                      radar= \"incendio\",\n",
        "                                      reshape_x = 2100,\n",
        "                                      reshape_y = 2100)\n",
        "df_real_test, lst_features_x, df_id = genera_dataframe(ruta_incendio = real_test_1,\n",
        "                                      ruta_no_incendio = real_test_0,\n",
        "                                      radar= \"incendio\",\n",
        "                                      reshape_x = 2100,\n",
        "                                      reshape_y = 2100)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAFAmlwi2pgf"
      },
      "outputs": [],
      "source": [
        "# DATOS DL\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "kernel_size = [2, 3, 5, 8]\n",
        "lst_lr = [0.001]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lado = 23 # Ancho y largo del lado de la imagen\n",
        "lst_neuronas = [32, 64, 128, 256]\n",
        "# lst_lr = [0.001, 0.01, 0.1]\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/1reales/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/1reales/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = validation_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqmHwieH7hPh"
      },
      "source": [
        "## Modelos ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H2lqZ3a7kC2"
      },
      "outputs": [],
      "source": [
        "# Generamos procesos previos:\n",
        "inicio = time.time()\n",
        "X_train, X_test, y_train, y_test = preprocesado(df_train = df_real_train, df_test = df_real_test)\n",
        "\n",
        "# REGRESIÓN LOGÍSTICA\n",
        "# Hiperparámetros\n",
        "C = [0.9, 0.8, 0.7] # Intensidad de la regularización\n",
        "regularization_methods = [\"l1\", \"l2\"] # Métodos de regularización\n",
        "max_iterations = [100, 1000, 10000, 10000] # Números máximos de iteraciones\n",
        "\n",
        "real_result_logistica = regresion_logistica(X_train = X_train,\n",
        "                                              X_test = X_test,\n",
        "                                              y_train = y_train,\n",
        "                                              y_test = y_test,\n",
        "                                              C = C, \n",
        "                                regularization_methods = regularization_methods,\n",
        "                                max_iterations = max_iterations)\n",
        "\n",
        "# XGBOOST\n",
        "# Hiperparámetros\n",
        "learning_rate = [0.001, 0.01, 0.1] # Tasa de aprendizaje\n",
        "max_depth = [5, 10, 20] # Profundidaz máxima\n",
        "n_estimators = [10, 50, 100, 200] # Número de árboles\n",
        "real_result_xgboost = func_xgboost(X_train = X_train,\n",
        "                                      X_test = X_test,\n",
        "                                      y_train = y_train,\n",
        "                                      y_test = y_test,\n",
        "                                      learning_rate = learning_rate,\n",
        "                                      max_depth = max_depth,\n",
        "                                      n_estimators = n_estimators)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1YpQTHi-zmD"
      },
      "outputs": [],
      "source": [
        "real_best_result_log = max(\n",
        "    real_result_logistica, key=lambda x: x['f1_score'])\n",
        "real_best_result_xgboost = max(\n",
        "    real_result_xgboost, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gLs6Py0-zp_"
      },
      "outputs": [],
      "source": [
        "real_best_result_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM5YAnDO-zvu"
      },
      "outputs": [],
      "source": [
        "real_best_result_xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RbOU1AW2Sxz"
      },
      "source": [
        "## Modelos de DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dskUanlnz1pN"
      },
      "outputs": [],
      "source": [
        "# Falta 0.01\n",
        "\n",
        "lst_lr = [0.001]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lado = 23 # Ancho y largo del lado de la imagen\n",
        "lst_neuronas = [256]\n",
        "resultados_red_densa = red_densa(train = train_ds,\n",
        "                                    test = validation_ds,\n",
        "                                    lst_neuronas = lst_neuronas, \n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    input_shape= lado*lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWBysQYH72Lr"
      },
      "outputs": [],
      "source": [
        "kernel_size = [2, 3, 5, 8]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lst_lr = [0.001, 0.01, 0.1]\n",
        "resultados_conv = red_convolucional(train = train_conv,\n",
        "                                    test = validation_conv,\n",
        "                                    kernel_size = kernel_size,\n",
        "                                    lst_lr = lst_lr,\n",
        "                                    lst_epoch = lst_epoch,\n",
        "                                    lado = lado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFGFcnfS7tWB"
      },
      "outputs": [],
      "source": [
        "real_best_result_rd = max(\n",
        "    resultados_red_densa, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF91Cuat8TKF"
      },
      "outputs": [],
      "source": [
        "resultados_conv_real = max(\n",
        "    resultados_conv, key=lambda x: x['f1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1EQHCIG8TKG"
      },
      "outputs": [],
      "source": [
        "real_best_result_rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1j3B19W8TKG"
      },
      "outputs": [],
      "source": [
        "real_best_result_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF58-_0B8DR1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94do4Q1n8Dc9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwtUN7W-z0Ah"
      },
      "source": [
        "# Coordenadas X e Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pZfKvdwzNEW"
      },
      "source": [
        "## Preparación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orAFpdC7zMMs"
      },
      "outputs": [],
      "source": [
        "# Se ha ejecutado tras el método unificado por lo que ya están creadas las variables\n",
        "df1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/total_incendios_control.csv', sep=',')\n",
        "df2 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/incendios_target3.csv', sep=',')\n",
        "df1 = df1[[\"X\",\"Y\",\"id\"]]\n",
        "df2 = df2[[\"X\",\"Y\",\"id\"]]\n",
        "df_coord = pd.concat([df1,df2], axis = 0)\n",
        "df_coord = df_coord.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpgFWN-T_ZOu"
      },
      "outputs": [],
      "source": [
        "df_train_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_0.csv', sep=',')\n",
        "df_train_coord_0 = pd.merge(df_coord, df_train_0, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_train_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_1.csv', sep=',')\n",
        "df_train_coord_1 = pd.merge(df_coord, df_train_1, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_train = pd.concat([df_train_0, df_train_1], axis=0)\n",
        "df_train = df_train.drop_duplicates()\n",
        "\n",
        "df_test_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_0.csv', sep=',')\n",
        "df_test_coord_0 = pd.merge(df_coord, df_test_0, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_test_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_1.csv', sep=',')\n",
        "df_test_coord_1 = pd.merge(df_coord, df_test_1, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_test = pd.concat([df_test_0, df_test_1], axis=0)\n",
        "df_test = df_test.drop_duplicates()\n",
        "\n",
        "df_train_coord_0[\"target\"] = 0\n",
        "df_train_coord_1[\"target\"] = 1\n",
        "df_test_coord_0[\"target\"] = 0\n",
        "df_test_coord_1[\"target\"] = 1\n",
        "df_coordenadas = pd.concat([df_train_coord_0, df_train_coord_1, df_test_coord_0, df_test_coord_1])\n",
        "df_coordenadas_train = pd.concat([df_train_coord_0, df_train_coord_1])\n",
        "df_coordenadas_test= pd.concat([df_test_coord_0, df_test_coord_1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQBmzfuXbTqg"
      },
      "outputs": [],
      "source": [
        "df_coordenadas.to_csv('/content/gdrive/My Drive/incendios_satelite/df_coordenadas.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xThZdjGj-1jG"
      },
      "source": [
        "## Modelos de ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJFl3-AR-0-E"
      },
      "outputs": [],
      "source": [
        "# Procesos previos\n",
        "# Separamos los datos\n",
        "var_x = [\"X\",\"Y\"]\n",
        "X_train = df_coordenadas_train[var_x]\n",
        "y_train = df_coordenadas_train[\"target\"]\n",
        "X_test = df_coordenadas_test[var_x]\n",
        "y_test = df_coordenadas_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Hiperparámetros\n",
        "C = [0.9, 0.8, 0.7] # Intensidad de la regularización\n",
        "regularization_methods = [\"l1\", \"l2\"] # Métodos de regularización\n",
        "max_iterations = [100, 1000, 10000, 10000, 10000] # Números máximos de iteraciones\n",
        "learning_rate = [0.001, 0.01, 0.1] # Tasa de aprendizaje\n",
        "max_depth = [5, 10, 20] # Profundidaz máxima\n",
        "n_estimators = [10, 50, 100, 200] # Número de árboles\n",
        "\n",
        "for lr in  learning_rate:\n",
        "  for md in max_depth:\n",
        "    for ne in n_estimators:\n",
        "      model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                            learning_rate = lr,\n",
        "                                            max_depth = md,\n",
        "                                            n_estimators=ne,\n",
        "                                            seed=112)\n",
        "\n",
        "      # Entrenamos el modelo\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      # Predecimos los datos de train\n",
        "      y_pred = model.predict(X_test)\n",
        "\n",
        "      # Calculamos los estadísticos\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "      precision = precision_score(y_test, y_pred)\n",
        "      recall = recall_score(y_test, y_pred)\n",
        "      f1 = f1_score(y_test, y_pred)\n",
        "      print(\"Para los valores XGBoost de learning rate {0}, max_depth {1} y n_estimator {2}, el valor de accuracy es: {3}, precision {4}, recall {5} y f1 score {6}\".format(\n",
        "          lr, md, ne, accuracy, precision, recall, f1\n",
        "      ))\n",
        "\n",
        "for c in  C:\n",
        "  for rm in regularization_methods:\n",
        "    for it in max_iterations:\n",
        "      log_reg = LogisticRegression(C=c,\n",
        "                                         penalty=rm,\n",
        "                                         solver='liblinear',\n",
        "                                         max_iter=it,\n",
        "                                         random_state =112)\n",
        "\t\t\t\t\t\t\t\t\t\t \n",
        "      # Entrena el modelo con el conjunto de entrenamiento\n",
        "      log_reg.fit(X_train, y_train)\n",
        "\n",
        "      # Realiza predicciones con el conjunto de prueba\n",
        "      y_pred = log_reg.predict(X_test)\n",
        "\n",
        "      # Calculamos los estadísticos\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "      precision = precision_score(y_test, y_pred)\n",
        "      recall = recall_score(y_test, y_pred)\n",
        "      f1 = f1_score(y_test, y_pred)\n",
        "      print(\"Para los valores de Regresión Logística de itraciones {0}, con valor de C {1} y regularización {2}, el valor de accuracy es: {3}, precision {4}, recall {5} y f1 score {6}\".format(\n",
        "          it, c, rm, accuracy, precision, recall, f1\n",
        "      ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gNmCJBPQUlu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB_rj-3ZF-eQ"
      },
      "source": [
        "## Modelo de DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPxhgKBnF9r_"
      },
      "outputs": [],
      "source": [
        "# Separamos las variables independientes de la variable dependiente\n",
        "var_x = [\"X\",\"Y\"]\n",
        "X_train = df_train[var_x]\n",
        "y_train = df_train[\"target\"]\n",
        "X_test = df_test[var_x]\n",
        "y_test = df_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00DGRvDrHmfZ"
      },
      "outputs": [],
      "source": [
        "lst_lr = [0.01, 0.1]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lst_neuronas = [32, 64, 128, 256]\n",
        "for neuronas in lst_neuronas:\n",
        "  for lr in lst_lr:\n",
        "    for epochs in lst_epoch:\n",
        "      model1 = Sequential()\n",
        "      model1.add(Dense(neuronas, activation='relu', input_shape=(2,)))\n",
        "      model1.add(Dropout(0.2))\n",
        "      model1.add(Dense(neuronas, activation='relu'))\n",
        "      model1.add(Dropout(0.2))\n",
        "      model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "      # Compile\n",
        "      loss_fn = keras.losses.BinaryCrossentropy()\n",
        "      opt = keras.optimizers.Adam(learning_rate = lr)\n",
        "      model1.compile(optimizer=opt, loss=loss_fn,\n",
        "        metrics=['accuracy',\n",
        "                tfa.metrics.F1Score(\n",
        "                num_classes=2,average=\"micro\",\n",
        "                threshold=0.75)])\n",
        "\n",
        "      model_dense = model1.fit(X_train, y_train, epochs=epochs, \n",
        "                               validation_data=(X_train, y_train),\n",
        "                               verbose = 0)\n",
        "                \n",
        "      val_accuracy = model_dense.history[\"val_accuracy\"][epochs-1]\n",
        "      val_f1_score = model_dense.history[\"val_f1_score\"][epochs-1]\n",
        "\n",
        "      print(\"learning rate {0}, epochs {1} neuronas {2} accuracy {3} y f1_score {4}\".format(\n",
        "        lr, epochs, neuronas, val_accuracy, val_f1_score\n",
        "                  ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hYZRt3zao4E"
      },
      "source": [
        "# Modelo únicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxEYRmIV6G8k"
      },
      "outputs": [],
      "source": [
        "kernel_size = [2, 3, 5, 8]\n",
        "lst_lr = [0.001, 0.01, 0.1]\n",
        "lst_epoch = [20, 50, 100, 200]\n",
        "lado = 23 # Ancho y largo del lado de la imagen\n",
        "lst_neuronas = [32, 64, 128, 256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B56xbNqAawsi"
      },
      "outputs": [],
      "source": [
        "import keras_metrics\n",
        "import keras\n",
        "\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/2lst/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/2lst/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "\n",
        "# Temperatura\n",
        "ks = 3\n",
        "lr = 0.001\n",
        "epochs = 200\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(16, kernel_size=(ks, ks), activation='relu',\n",
        "                              input_shape=(lado, lado, 1)))\n",
        "model2.add(Conv2D(32, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile\n",
        "loss_fn = keras.losses.BinaryCrossentropy()\n",
        "opt = keras.optimizers.Adam(learning_rate = lr)\n",
        "model2.compile(optimizer=opt, loss=loss_fn,\n",
        "                            metrics=['accuracy',\n",
        "                                    tfa.metrics.F1Score(\n",
        "                                        num_classes=2,average=\"micro\",\n",
        "                                        threshold=0.75),\n",
        "                                     keras.metrics.Precision(),\n",
        "                                     keras.metrics.Recall(),\n",
        "                                     keras.metrics.AUC()])\n",
        "tfa.metrics.multilabel_confusion_matrix\n",
        "                                                     \n",
        "model_conv = model2.fit(train_conv,\n",
        "                                    epochs=epochs, \n",
        "                                     validation_data=validation_conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mbg_EpHhhYY"
      },
      "outputs": [],
      "source": [
        "import keras_metrics\n",
        "import keras\n",
        "lado = 23\n",
        "# Humedad relativa\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/3nvdi/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/3nvdi/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "\n",
        "ks = 3\n",
        "lr = 0.001\n",
        "epochs = 200\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(16, kernel_size=(ks, ks), activation='relu',\n",
        "                              input_shape=(lado, lado, 1)))\n",
        "model2.add(Conv2D(32, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile\n",
        "loss_fn = keras.losses.BinaryCrossentropy()\n",
        "opt = keras.optimizers.Adam(learning_rate = lr)\n",
        "model2.compile(optimizer=opt, loss=loss_fn,\n",
        "                            metrics=['accuracy',\n",
        "                                    tfa.metrics.F1Score(\n",
        "                                        num_classes=2,average=\"micro\",\n",
        "                                        threshold=0.75),\n",
        "                                     keras.metrics.Precision(),\n",
        "                                     keras.metrics.Recall(),\n",
        "                                     keras.metrics.AUC()])\n",
        "tfa.metrics.multilabel_confusion_matrix\n",
        "                                                     \n",
        "model_conv = model2.fit(train_conv,\n",
        "                                    epochs=epochs, \n",
        "                                     validation_data=validation_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PNqXFz0rcr7"
      },
      "outputs": [],
      "source": [
        "model2.save('/content/gdrive/My Drive/incendios_satelite/humedad.h5')\n",
        "plot_prediction(200,model_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJAMtEkq1qez"
      },
      "outputs": [],
      "source": [
        "import keras_metrics\n",
        "import keras\n",
        "# Población\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/4poblacion/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/4poblacion/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "\n",
        "ks = 5\n",
        "lr = 0.001\n",
        "epochs = 200\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(16, kernel_size=(ks, ks), activation='relu',\n",
        "                              input_shape=(lado, lado, 1)))\n",
        "model2.add(Conv2D(32, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile\n",
        "loss_fn = keras.losses.BinaryCrossentropy()\n",
        "opt = keras.optimizers.Adam(learning_rate = lr)\n",
        "model2.compile(optimizer=opt, loss=loss_fn,\n",
        "                            metrics=['accuracy',\n",
        "                                    tfa.metrics.F1Score(\n",
        "                                        num_classes=2,average=\"micro\",\n",
        "                                        threshold=0.75),\n",
        "                                     keras.metrics.Precision(),\n",
        "                                     keras.metrics.Recall(),\n",
        "                                     keras.metrics.AUC()])\n",
        "tfa.metrics.multilabel_confusion_matrix\n",
        "                                                     \n",
        "model_conv = model2.fit(train_conv,\n",
        "                                    epochs=epochs, \n",
        "                                     validation_data=validation_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8bpq9p53QfF"
      },
      "outputs": [],
      "source": [
        "model2.save('/content/gdrive/My Drive/incendios_satelite/poblcacion.h5')\n",
        "plot_prediction(200,model_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z75MOY2e5UyT"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "lado = 23\n",
        "\n",
        "# Imagen real\n",
        "train_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/1reales/training_data/',\n",
        "    labels='inferred', \n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "validation_conv = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/gdrive/My Drive/incendios_satelite/1reales/validation_data/',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    color_mode = 'grayscale',\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    image_size=(lado, lado))\n",
        "\n",
        "validation_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "train_conv = train_conv.map(lambda x, y: (tensorflow.divide(x, 255), y))\n",
        "\n",
        "train_ds = train_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "validation_ds = validation_conv.map(lambda x, y: (keras.layers.Flatten()(x), y))\n",
        "\n",
        "\n",
        "ks = 2\n",
        "lr = 0.001\n",
        "epochs = 200\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(16, kernel_size=(ks, ks), activation='relu',\n",
        "                              input_shape=(lado, lado, 1)))\n",
        "model2.add(Conv2D(32, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(Conv2D(64, (ks, ks), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile\n",
        "loss_fn = keras.losses.BinaryCrossentropy()\n",
        "opt = keras.optimizers.Adam(learning_rate = lr)\n",
        "model2.compile(optimizer=opt, loss=loss_fn,\n",
        "                            metrics=['accuracy',\n",
        "                                    tfa.metrics.F1Score(\n",
        "                                        num_classes=2,average=\"micro\",\n",
        "                                        threshold=0.75),\n",
        "                                     keras.metrics.Precision(),\n",
        "                                     keras.metrics.Recall(),\n",
        "                                     keras.metrics.AUC()])\n",
        "tfa.metrics.multilabel_confusion_matrix\n",
        "                                                     \n",
        "model_conv = model2.fit(train_conv,\n",
        "                                    epochs=epochs, \n",
        "                                     validation_data=validation_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C695Coe7Sjt"
      },
      "outputs": [],
      "source": [
        "model2.save('/content/gdrive/My Drive/incendios_satelite/real.h5')\n",
        "plot_prediction(200,model_conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsvowFJ4Uwil"
      },
      "source": [
        "# Union de DF entrenamiento y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCEYJ--8W33F"
      },
      "outputs": [],
      "source": [
        "path_ruta = '/content/gdrive/My Drive/incendios_satelite/'\n",
        "path_temperatura = '2lst'\n",
        "path_humedad = '3nvdi'\n",
        "path_poblacion = '4poblacion'\n",
        "path_real = '1reales'\n",
        "path_entrenamiento = 'training_data'\n",
        "path_test = 'validation_data'\n",
        "path_clase_0 = 'class_0'\n",
        "path_clase_1 = 'class_1'\n",
        "\n",
        "lst_variables = [path_temperatura, path_humedad, path_poblacion, path_real]\n",
        "lst_tipo = [path_entrenamiento, path_test]\n",
        "lst_clase = [path_clase_0, path_clase_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxJV0R-MWRrS"
      },
      "outputs": [],
      "source": [
        "inicio = time.time()\n",
        "for tipo in lst_tipo:\n",
        "  for clase in lst_clase:\n",
        "    temperatura =  []\n",
        "    humedad = []\n",
        "    poblacion = []\n",
        "    real = []\n",
        "    for file in os.listdir(path_ruta + lst_variables[0] + '/' + tipo + '/' + clase + '/'):\n",
        "        if \"jpeg\" in file:\n",
        "          id_imagen = int(file.split(\"L\")[0])\n",
        "          temperatura.append(id_imagen)\n",
        "    for file in os.listdir(path_ruta + lst_variables[1] + '/' + tipo + '/' + clase + '/'):\n",
        "        if \"jpeg\" in file:\n",
        "          id_imagen = int(file.split(\"N\")[0])\n",
        "          humedad.append(id_imagen)\n",
        "    for file in os.listdir(path_ruta + lst_variables[2] + '/' + tipo + '/' + clase + '/'):\n",
        "        if \"jpeg\" in file:\n",
        "          id_imagen = int(file.split(\"s\")[0])\n",
        "          poblacion.append(id_imagen)\n",
        "    for file in os.listdir(path_ruta + lst_variables[3] + '/' + tipo + '/' + clase + '/'):\n",
        "        if \"jpeg\" in file:\n",
        "          id_imagen = int(file.split(\"i\")[0])\n",
        "          real.append(id_imagen)\n",
        "        \n",
        "    # Genero los dataframes\n",
        "    df_temperatura = pd.DataFrame(data = temperatura, columns =[\"id\"] )\n",
        "    df_humedad = pd.DataFrame(data = humedad, columns =[\"id\"] )\n",
        "    df_poblacion = pd.DataFrame(data = poblacion, columns =[\"id\"] )\n",
        "    df_real = pd.DataFrame(data = real, columns =[\"id\"] )\n",
        "\n",
        "    # Me quedo con los que estén en todos\n",
        "    df_cruce = pd.merge(df_temperatura, df_humedad, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "    df_cruce = pd.merge(df_cruce, df_poblacion, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "    df_cruce = pd.merge(df_cruce, df_real, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "    df_cruce = df_cruce.drop_duplicates()\n",
        "\n",
        "    # Escribo en csv para poder reutilizarlo\n",
        "    archivo = 'df_' + tipo + clase + '.csv'\n",
        "    print(\"Guardo el fichero {0} que contiene {1} registros\".format(archivo, len(df_cruce)))\n",
        "    df_cruce.to_csv('/content/gdrive/My Drive/incendios_satelite/' + archivo, index=False)\n",
        "\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5amoaQDie2wf"
      },
      "source": [
        "## Temperatura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH1zgfxPYvDv"
      },
      "outputs": [],
      "source": [
        "df_train_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_0.csv', sep=',')\n",
        "df_train_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_1.csv', sep=',')\n",
        "df_train = pd.concat([df_train_0, df_train_1], axis=0)\n",
        "df_train = df_train.drop_duplicates()\n",
        "\n",
        "df_test_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_0.csv', sep=',')\n",
        "df_test_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_1.csv', sep=',')\n",
        "df_test = pd.concat([df_test_0, df_test_1], axis=0)\n",
        "df_test = df_test.drop_duplicates()\n",
        "\n",
        "# DATOS DL\n",
        "# Generamos el dataframe de temperatura\n",
        "inicio = time.time()\n",
        "path_no_incendios = path_ruta + lst_variables[0] + '/' + lst_tipo[0] + '/' + lst_clase[0] + '/'\n",
        "path_incendios = path_ruta + lst_variables[0] + '/' + lst_tipo[0] + '/' + lst_clase[1] + '/'\n",
        "path_no_incendios_test = path_ruta + lst_variables[0] + '/' + lst_tipo[1] + '/' + lst_clase[0] + '/'\n",
        "path_incendios_test = path_ruta + lst_variables[0] + '/' + lst_tipo[1] + '/' + lst_clase[1] + '/'\n",
        "print(path_incendios_test)\n",
        "df_temperatura, lst_features_x, df_id = genera_dataframe(ruta_incendio = path_incendios,\n",
        "                                      ruta_no_incendio = path_no_incendios,\n",
        "                                      radar= \"LST_Day\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "\n",
        "df_temperatura_test, lst_features_x, df_id_test = genera_dataframe(ruta_incendio = path_incendios_test,\n",
        "                                      ruta_no_incendio = path_no_incendios_test,\n",
        "                                      radar= \"LST_Day\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlrDWeYS9Z9B"
      },
      "outputs": [],
      "source": [
        "# Procesos previos\n",
        "# Separamos los datos\n",
        "X_train = df_temperatura[lst_features_x]\n",
        "y_train = df_temperatura[\"target\"]\n",
        "X_test = df_temperatura_test[lst_features_x]\n",
        "y_test = df_temperatura_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                      learning_rate = 0.1,\n",
        "                                      max_depth = 5,\n",
        "                                      n_estimators=200,\n",
        "                                      seed=112)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecimos los datos de train\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculamos los estadísticos\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"El valor de accuracy es: {0}, precision {1}, recall {2} y f1 score {3}\".format(\n",
        "    accuracy, precision, recall, f1\n",
        "))\n",
        "df_temperatura_test['predicciones_temp'] = y_pred\n",
        "df_temperatura_test['target_temp'] = y_test\n",
        "df_temperatura_test = df_temperatura_test[[\"id\",\"predicciones_temp\",\"target_temp\"]]\n",
        "df_temperatura_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIrjgY5zqOAl"
      },
      "outputs": [],
      "source": [
        "df_temperatura_test = pd.merge(df_temperatura_test, df_test, how = 'inner', left_on = 'id', right_on = 'id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0K5vEkmdLKq"
      },
      "outputs": [],
      "source": [
        "df_temperatura_test.to_csv('/content/gdrive/My Drive/incendios_satelite/df_temperatura_XGBoost.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQaCGwvFe5i2"
      },
      "source": [
        "##  Humedad relativa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9soIJYJH2vvH"
      },
      "outputs": [],
      "source": [
        "df_train_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_0.csv', sep=',')\n",
        "df_train_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_1.csv', sep=',')\n",
        "df_train = pd.concat([df_train_0, df_train_1], axis=0)\n",
        "df_train = df_train.drop_duplicates()\n",
        "\n",
        "df_test_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_0.csv', sep=',')\n",
        "df_test_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_1.csv', sep=',')\n",
        "df_test = pd.concat([df_test_0, df_test_1], axis=0)\n",
        "df_test = df_test.drop_duplicates()\n",
        "\n",
        "# DATOS DL\n",
        "# Generamos el dataframe de humedad relativa\n",
        "inicio = time.time()\n",
        "path_no_incendios = path_ruta + lst_variables[1] + '/' + lst_tipo[0] + '/' + lst_clase[0] + '/'\n",
        "path_incendios = path_ruta + lst_variables[1] + '/' + lst_tipo[0] + '/' + lst_clase[1] + '/'\n",
        "path_no_incendios_test = path_ruta + lst_variables[1] + '/' + lst_tipo[1] + '/' + lst_clase[0] + '/'\n",
        "path_incendios_test = path_ruta + lst_variables[1] + '/' + lst_tipo[1] + '/' + lst_clase[1] + '/'\n",
        "print(path_incendios_test)\n",
        "df_humedad, lst_features_x, df_id = genera_dataframe(ruta_incendio = path_incendios,\n",
        "                                      ruta_no_incendio = path_no_incendios,\n",
        "                                      radar= \"NDVI\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "\n",
        "df_humedad_test, lst_features_x, df_id_test = genera_dataframe(ruta_incendio = path_incendios_test,\n",
        "                                      ruta_no_incendio = path_no_incendios_test,\n",
        "                                      radar= \"NDVI\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDls97Aqgrxf"
      },
      "outputs": [],
      "source": [
        "# Procesos previos\n",
        "# Separamos los datos\n",
        "X_train = df_humedad[lst_features_x]\n",
        "y_train = df_humedad[\"target\"]\n",
        "X_test = df_humedad_test[lst_features_x]\n",
        "y_test = df_humedad_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                      learning_rate = 0.1,\n",
        "                                      max_depth = 5,\n",
        "                                      n_estimators=200,\n",
        "                                      seed=112)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecimos los datos de train\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculamos los estadísticos\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"El valor de accuracy es: {0}, precision {1}, recall {2} y f1 score {3}\".format(\n",
        "    accuracy, precision, recall, f1\n",
        "))\n",
        "df_humedad_test['predicciones_hum'] = y_pred\n",
        "df_humedad_test['target_hum'] = y_test\n",
        "df_humedad_test = df_humedad_test[[\"id\",\"predicciones_hum\",\"target_hum\"]]\n",
        "df_humedad_test = pd.merge(df_humedad_test, df_test, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_humedad_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urjAJv-Nh6l4"
      },
      "outputs": [],
      "source": [
        "df_humedad_test.to_csv('/content/gdrive/My Drive/incendios_satelite/df_humedad_XGBoost.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu3vKPn-iRIB"
      },
      "source": [
        "## Poblacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rp_iyjFhkuM"
      },
      "outputs": [],
      "source": [
        "df_train_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_0.csv', sep=',')\n",
        "df_train_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_1.csv', sep=',')\n",
        "df_train = pd.concat([df_train_0, df_train_1], axis=0)\n",
        "df_train = df_train.drop_duplicates()\n",
        "\n",
        "df_test_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_0.csv', sep=',')\n",
        "df_test_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_1.csv', sep=',')\n",
        "df_test = pd.concat([df_test_0, df_test_1], axis=0)\n",
        "df_test = df_test.drop_duplicates()\n",
        "\n",
        "# DATOS DL\n",
        "# Generamos el dataframe de temperatura\n",
        "inicio = time.time()\n",
        "path_no_incendios = path_ruta + lst_variables[2] + '/' + lst_tipo[0] + '/' + lst_clase[0] + '/'\n",
        "path_incendios = path_ruta + lst_variables[2] + '/' + lst_tipo[0] + '/' + lst_clase[1] + '/'\n",
        "path_no_incendios_test = path_ruta + lst_variables[2] + '/' + lst_tipo[1] + '/' + lst_clase[0] + '/'\n",
        "path_incendios_test = path_ruta + lst_variables[2] + '/' + lst_tipo[1] + '/' + lst_clase[1] + '/'\n",
        "print(path_incendios_test)\n",
        "df_poblacion, lst_features_x, df_id = genera_dataframe(ruta_incendio = path_incendios,\n",
        "                                      ruta_no_incendio = path_no_incendios,\n",
        "                                      radar= \"smod\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "\n",
        "df_poblacion_test, lst_features_x, df_id_test = genera_dataframe(ruta_incendio = path_incendios_test,\n",
        "                                      ruta_no_incendio = path_no_incendios_test,\n",
        "                                      radar= \"smod\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kvpg69p4izrg"
      },
      "outputs": [],
      "source": [
        "# Procesos previos\n",
        "# Separamos los datos\n",
        "X_train = df_poblacion[lst_features_x]\n",
        "y_train = df_poblacion[\"target\"]\n",
        "X_test = df_poblacion_test[lst_features_x]\n",
        "y_test = df_poblacion_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                      learning_rate = 0.1,\n",
        "                                      max_depth = 5,\n",
        "                                      n_estimators=300,\n",
        "                                      seed=112)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecimos los datos de train\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculamos los estadísticos\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"El valor de accuracy es: {0}, precision {1}, recall {2} y f1 score {3}\".format(\n",
        "    accuracy, precision, recall, f1\n",
        "))\n",
        "df_poblacion_test['predicciones_pob'] = y_pred\n",
        "df_poblacion_test['target_pob'] = y_test\n",
        "df_poblacion_test = df_poblacion_test[[\"id\",\"predicciones_pob\",\"target_pob\"]]\n",
        "df_poblacion_test = pd.merge(df_poblacion_test, df_test, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_poblacion_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDpicPBqu-pP"
      },
      "outputs": [],
      "source": [
        "df_poblacion_test.to_csv('/content/gdrive/My Drive/incendios_satelite/df_poblacion_XGBoost.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf1lMnltlPb1"
      },
      "source": [
        "## Real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vS7fss9lCMf"
      },
      "outputs": [],
      "source": [
        "df_train_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_0.csv', sep=',')\n",
        "df_train_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_training_dataclass_1.csv', sep=',')\n",
        "df_train = pd.concat([df_train_0, df_train_1], axis=0)\n",
        "df_train = df_train.drop_duplicates()\n",
        "\n",
        "df_test_0 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_0.csv', sep=',')\n",
        "df_test_1 = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_validation_dataclass_1.csv', sep=',')\n",
        "df_test = pd.concat([df_test_0, df_test_1], axis=0)\n",
        "df_test = df_test.drop_duplicates()\n",
        "\n",
        "# DATOS DL\n",
        "# Generamos el dataframe de temperatura\n",
        "inicio = time.time()\n",
        "path_no_incendios = path_ruta + lst_variables[3] + '/' + lst_tipo[0] + '/' + lst_clase[0] + '/'\n",
        "path_incendios = path_ruta + lst_variables[3] + '/' + lst_tipo[0] + '/' + lst_clase[1] + '/'\n",
        "path_no_incendios_test = path_ruta + lst_variables[3] + '/' + lst_tipo[1] + '/' + lst_clase[0] + '/'\n",
        "path_incendios_test = path_ruta + lst_variables[3] + '/' + lst_tipo[1] + '/' + lst_clase[1] + '/'\n",
        "print(path_incendios_test)\n",
        "\n",
        "df_real, lst_features_x, df_id = genera_dataframe(ruta_incendio = path_incendios,\n",
        "                                      ruta_no_incendio = path_no_incendios,\n",
        "                                      radar= \"incendio\",\n",
        "                                      reshape_x = 2100,\n",
        "                                      reshape_y = 2100)\n",
        "\n",
        "df_real_test, lst_features_x, df_id_test = genera_dataframe(ruta_incendio = path_incendios_test,\n",
        "                                      ruta_no_incendio = path_no_incendios_test,\n",
        "                                      radar= \"incendio\",\n",
        "                                      reshape_x = 2100,\n",
        "                                      reshape_y = 2100)\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A9My0nhl5qL"
      },
      "outputs": [],
      "source": [
        "# Procesos previos\n",
        "# Separamos los datos\n",
        "X_train = df_real[lst_features_x]\n",
        "y_train = df_real[\"target\"]\n",
        "X_test = df_real_test[lst_features_x]\n",
        "y_test = df_real_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                      learning_rate = 0.1,\n",
        "                                      max_depth = 20,\n",
        "                                      n_estimators=200,\n",
        "                                      seed=112)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecimos los datos de train\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculamos los estadísticos\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"El valor de accuracy es: {0}, precision {1}, recall {2} y f1 score {3}\".format(\n",
        "    accuracy, precision, recall, f1\n",
        "))\n",
        "df_real_test['predicciones_real'] = y_pred\n",
        "df_real_test['target_real'] = y_test\n",
        "df_real_test = df_real_test[[\"id\",\"predicciones_real\",\"target_real\"]]\n",
        "df_real_test = pd.merge(df_real_test, df_test, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_real_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM76eWnCGf7A"
      },
      "outputs": [],
      "source": [
        "df_real_test.to_csv('/content/gdrive/My Drive/incendios_satelite/df_poblacion_real.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umb9_fPLSyRN"
      },
      "source": [
        "## Viento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ7EyjLxS0-y"
      },
      "outputs": [],
      "source": [
        "df_viento = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_viento.csv', sep=',')\n",
        "df_viento = df_viento.drop(columns=[\"target\"])\n",
        "df_viento = df_viento.rename(columns={\"col_x\": \"vel_viento\"})\n",
        "df_viento_test = pd.merge(df_viento, df_test, how = 'inner', left_on = 'id', right_on = 'id')\n",
        "df_viento_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwAGcdXxa8xB"
      },
      "source": [
        "## Coordenadas X e Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI0Yp4EAb3jA"
      },
      "outputs": [],
      "source": [
        "# Procesos previos\n",
        "# Separamos los datos\n",
        "var_x = [\"X\",\"Y\"]\n",
        "X_train = df_coordenadas_train[var_x]\n",
        "y_train = df_coordenadas_train[\"target\"]\n",
        "X_test = df_coordenadas_test[var_x]\n",
        "y_test = df_coordenadas_test[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "lr = 0.1\n",
        "md = 20\n",
        "ne = 10\n",
        "\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                            learning_rate = lr,\n",
        "                                            max_depth = md,\n",
        "                                            n_estimators=ne,\n",
        "                                            seed=112)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecimos los datos de train\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculamos los estadísticos\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f1)\n",
        "print(\"Para los valores XGBoost de learning rate {0}, max_depth {1} y n_estimator {2}, el valor de accuracy es: {3}, precision {4}, recall {5} y f1 score {6}\".format(\n",
        "          lr, md, ne, accuracy, precision, recall, f1\n",
        "      ))\n",
        "\n",
        "\n",
        "df_coordenadas_test['predicciones_coord'] = y_pred\n",
        "df_coordenadas_test['target_coord'] = y_test\n",
        "df_coordenadas_test = df_coordenadas_test[[\"id\",\"predicciones_coord\",\"target_coord\"]]\n",
        "df_coordenadas_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES7fI3w4TXHv"
      },
      "outputs": [],
      "source": [
        "df_coordenadas_test.to_csv('/content/gdrive/My Drive/incendios_satelite/df_coordenadas_real.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnm23Ai4WyJT"
      },
      "outputs": [],
      "source": [
        "# Calculamos los estadísticos\n",
        "accuracy = accuracy_score(df_coordenadas_test[\"target_coord\"], df_coordenadas_test[\"predicciones_coord\"])\n",
        "precision = precision_score(df_coordenadas_test[\"target_coord\"], df_coordenadas_test[\"predicciones_coord\"])\n",
        "recall = recall_score(df_coordenadas_test[\"target_coord\"], df_coordenadas_test[\"predicciones_coord\"])\n",
        "f1 = f1_score(df_coordenadas_test[\"target_coord\"], df_coordenadas_test[\"predicciones_coord\"])\n",
        "print(\"El valor de accuracy es: {0}, precision {1}, recall {2} y f1 score {3}\".format(\n",
        "    accuracy, precision, recall, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdO3BOXUm1_0"
      },
      "source": [
        "## Puertas lógicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVqbqYm7TjSo"
      },
      "outputs": [],
      "source": [
        "df_temperatura_test = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_temperatura_XGBoost.csv', sep=',')\n",
        "df_humedad_test = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_humedad_XGBoost.csv', sep=',')\n",
        "df_poblacion_test = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_poblacion_XGBoost.csv', sep=',')\n",
        "df_real_test = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_poblacion_real.csv', sep=',')\n",
        "df_coordenadas_test = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_coordenadas_real.csv', sep=',')\n",
        "df_viento_test = pd.read_csv('/content/gdrive/My Drive/incendios_satelite/df_viento.csv', sep=',')\n",
        "\n",
        "df_temperatura_test = df_temperatura_test.drop_duplicates()\n",
        "df_humedad_test = df_humedad_test.drop_duplicates()\n",
        "df_poblacion_test = df_poblacion_test.drop_duplicates()\n",
        "df_real_test = df_real_test.drop_duplicates()\n",
        "df_coordenadas_test = df_coordenadas_test.drop_duplicates()\n",
        "df_viento_test = df_viento_test.drop_duplicates()\n",
        "\n",
        "items_a_excluir = [16941,18800]\n",
        "df_temperatura_test = df_temperatura_test[~df_temperatura_test['id'].isin(items_a_excluir)]\n",
        "df_humedad_test = df_humedad_test[~df_humedad_test['id'].isin(items_a_excluir)]\n",
        "df_poblacion_test = df_poblacion_test[~df_poblacion_test['id'].isin(items_a_excluir)]\n",
        "df_real_test = df_real_test[~df_real_test['id'].isin(items_a_excluir)]\n",
        "df_coordenadas_test = df_coordenadas_test[~df_coordenadas_test['id'].isin(items_a_excluir)]\n",
        "df_viento_test = df_viento_test[~df_viento_test['id'].isin([16941,18800,17238])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crF70Q-Qiuxu"
      },
      "outputs": [],
      "source": [
        "print(len(df_temperatura_test))\n",
        "print(len(df_humedad_test))\n",
        "print(len(df_poblacion_test))\n",
        "print(len(df_real_test))\n",
        "print(len(df_coordenadas_test))\n",
        "print(len(df_viento_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8FHLw-YVhwW"
      },
      "outputs": [],
      "source": [
        "print(f1_score(df_temperatura_test[\"target_temp\"], df_temperatura_test[\"predicciones_temp\"]))\n",
        "print(roc_auc_score(df_temperatura_test[\"target_temp\"], df_temperatura_test[\"predicciones_temp\"]))\n",
        "print(recall_score(df_temperatura_test[\"target_temp\"], df_temperatura_test[\"predicciones_temp\"]))\n",
        "print(precision_score(df_temperatura_test[\"target_temp\"], df_temperatura_test[\"predicciones_temp\"]))\n",
        "print(accuracy_score (df_temperatura_test[\"target_temp\"], df_temperatura_test[\"predicciones_temp\"]))\n",
        "cm = confusion_matrix(df_temperatura_test[\"target_temp\"], df_temperatura_test[\"predicciones_temp\"])\n",
        "class_names = ['Incendio', 'No incendio',]\n",
        "\n",
        "conf_mat_df = pd.DataFrame(cm, index=['Verdaderos negativos', 'Falsos positivos'], columns=['Falsos negativos', 'Verdaderos positivos'])\n",
        "\n",
        "sns.heatmap(conf_mat_df, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzacTrVGX94v"
      },
      "outputs": [],
      "source": [
        "print(f1_score(df_humedad_test[\"target_hum\"], df_humedad_test[\"predicciones_hum\"]))\n",
        "print(roc_auc_score (df_humedad_test[\"target_hum\"], df_humedad_test[\"predicciones_hum\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH3nWM3XYD59"
      },
      "outputs": [],
      "source": [
        "print(f1_score(df_poblacion_test[\"target_pob\"], df_poblacion_test[\"predicciones_pob\"]))\n",
        "print(roc_auc_score(df_poblacion_test[\"target_pob\"], df_poblacion_test[\"predicciones_pob\"]))\n",
        "print(recall_score(df_poblacion_test[\"target_pob\"], df_poblacion_test[\"predicciones_pob\"]))\n",
        "print(precision_score(df_poblacion_test[\"target_pob\"], df_poblacion_test[\"predicciones_pob\"]))\n",
        "print(accuracy_score (df_poblacion_test[\"target_pob\"], df_poblacion_test[\"predicciones_pob\"]))\n",
        "print(classification_report(df_poblacion_test[\"target_pob\"], df_poblacion_test[\"predicciones_pob\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2g1JbaJYL9o"
      },
      "outputs": [],
      "source": [
        "print(f1_score(df_real_test[\"target_real\"], df_real_test[\"predicciones_real\"]))\n",
        "print(roc_auc_score (df_real_test[\"target_real\"], df_real_test[\"predicciones_real\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz4EKpyDYSta"
      },
      "outputs": [],
      "source": [
        "print(f1_score(df_coordenadas_test[\"target_coord\"], df_coordenadas_test[\"predicciones_coord\"]))\n",
        "print(roc_auc_score(df_coordenadas_test[\"target_coord\"], df_coordenadas_test[\"predicciones_coord\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij7FH1Jqm4g8"
      },
      "outputs": [],
      "source": [
        "df_conjunto = pd.merge(df_temperatura_test, df_humedad_test, how = 'inner',\n",
        "                       left_on = 'id', right_on = 'id')\n",
        "df_conjunto = pd.merge(df_conjunto, df_poblacion_test, how = 'inner',\n",
        "                       left_on = 'id', right_on = 'id')\n",
        "df_conjunto = pd.merge(df_conjunto, df_real_test, how = 'inner',\n",
        "                       left_on = 'id', right_on = 'id')\n",
        "df_conjunto = pd.merge(df_conjunto, df_viento_test, how = 'inner',\n",
        "                       left_on = 'id', right_on = 'id')\n",
        "df_conjunto = pd.merge(df_conjunto, df_coordenadas_test, how = 'inner',\n",
        "                       left_on = 'id', right_on = 'id')\n",
        "df_conjunto[\"target\"] = df_conjunto[\"target_temp\"]\n",
        "df_conjunto[\"vel_viento\"] = (df_conjunto[\"col_x\"] - 149)/76 # Estandarizo\n",
        "df_conjunto = df_conjunto[[\"id\",\"predicciones_temp\",\"predicciones_hum\",\n",
        "                           \"predicciones_pob\",\"predicciones_real\",\n",
        "                           \"predicciones_coord\", \"vel_viento\",\"target\"]]\n",
        "df_conjunto[\"suma\"] = df_conjunto[\"predicciones_temp\"] + df_conjunto[\"predicciones_hum\"] +\\\n",
        "                      df_conjunto[\"predicciones_pob\"] + df_conjunto[\"predicciones_real\"] + \\\n",
        "                      df_conjunto[\"predicciones_coord\"]\n",
        "\n",
        "df_conjunto.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILGJQ9RZdmZ_"
      },
      "outputs": [],
      "source": [
        "print(f1_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_temp\"]))\n",
        "print(f1_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_hum\"]))\n",
        "print(f1_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_pob\"]))\n",
        "print(f1_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_real\"]))\n",
        "print(f1_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_coord\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoJ2n7cOoZwz"
      },
      "outputs": [],
      "source": [
        "# Calculamos los estadísticos\n",
        "for i in range(5):\n",
        "  df_conjunto[\"predicciones_agrup\"] = df_conjunto[\"suma\"].apply(lambda x: 1 if x>=i else 0)\n",
        "  accuracy = accuracy_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_agrup\"])\n",
        "  precision = precision_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_agrup\"])\n",
        "  recall = recall_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_agrup\"])\n",
        "  f1 = f1_score(df_conjunto[\"target\"], df_conjunto[\"predicciones_agrup\"])\n",
        "  print(\"Para {0} variables, el valor de accuracy es: {1}, precision {2}, recall {3} y f1 score {4}\".format(\n",
        "      i+1, accuracy, precision, recall, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1qFZVy9vahh"
      },
      "source": [
        "## Modelo agrupado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqXn1kM0ul1L"
      },
      "outputs": [],
      "source": [
        "x_var = [\"predicciones_pob\"]\n",
        "\n",
        "# x_var = [\"predicciones_pob\",        \"predicciones_pob\",\"predicciones_coord\",\"vel_viento\"]\n",
        "X = df_conjunto[x_var]\n",
        "y = df_conjunto[\"target\"]\n",
        "lst_f1 = []\n",
        "for i in range(1000):\n",
        "  # Dividimos los datos en entrenamiento y test\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(X_train)\n",
        "  X_train = scaler.transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  \n",
        "  model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                        learning_rate = 0.01,\n",
        "                                        max_depth = 20,\n",
        "                                        n_estimators=100,\n",
        "                                        seed=112)\n",
        "\n",
        "  # Entrenamos el modelo\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Predecimos los datos de test\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Calculamos los estadísticos\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  \n",
        "  lst_f1.append(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0h69wHSyA7w"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "mean(lst_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdchgfe5vpeb"
      },
      "outputs": [],
      "source": [
        "x_var = [\"predicciones_temp\",\"predicciones_hum\",\"predicciones_pob\",\n",
        "         \"predicciones_pob\",\"predicciones_coord\",\"vel_viento\"]\n",
        "X = df_conjunto[x_var]\n",
        "y = df_conjunto[\"target\"]\n",
        "\n",
        "# Dividimos los datos en entrenamiento y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Hiperparámetros\n",
        "learning_rate = [0.001, 0.01, 0.1] # Tasa de aprendizaje\n",
        "max_depth = [5, 10, 20] # Profundidaz máxima\n",
        "n_estimators = [10, 50, 100, 200] # Número de árboles\n",
        "\n",
        "for lr in  learning_rate:\n",
        "  for md in max_depth:\n",
        "    for ne in n_estimators:\n",
        "      model = xgb.XGBClassifier(objective='binary:logistic', \n",
        "                                            learning_rate = lr,\n",
        "                                            max_depth = md,\n",
        "                                            n_estimators=ne,\n",
        "                                            seed=112)\n",
        "\n",
        "      # Entrenamos el modelo\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      # Predecimos los datos de train\n",
        "      y_pred = model.predict(X_test)\n",
        "\n",
        "      # Calculamos los estadísticos\n",
        "      accuracy = accuracy_score(y_test, y_pred)\n",
        "      precision = precision_score(y_test, y_pred)\n",
        "      recall = recall_score(y_test, y_pred)\n",
        "      f1 = f1_score(y_test, y_pred)\n",
        "      print(\"Para los valores XGBoost de learning rate {0}, max_depth {1} y n_estimator {2}, el valor de accuracy es: {3}, precision {4}, recall {5} y f1 score {6}\".format(\n",
        "          lr, md, ne, accuracy, precision, recall, f1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XegOvIJHw39P"
      },
      "outputs": [],
      "source": [
        "def genera_dataframe(ruta_incendio, ruta_no_incendio, radar, reshape_x,\n",
        "                     reshape_y):\n",
        "    \n",
        "  \"\"\"\n",
        "  Esta función parte de las rutas en la que se les haya marcado incendios y no\n",
        "  incendios y devuelve un dataframe con el ancho solicitado.\n",
        "\n",
        "  Variables entrada:\n",
        "    ruta_incendio: Carpeta donde se encuentran las imágenes de incendio\n",
        "    ruta_no_incendio: Carpeta donde se encuentran las imágenes de no incendio\n",
        "    radar: tipo de radar (LST_Day, NDVI, et...)\n",
        "    reshape_x: número de píxeles de ancho que se espera.\n",
        "    reshape_y: número de píxeles de alto que se espera.\n",
        "\n",
        "  Ejemplo de llamada:\n",
        "    df_temperatura = genera_dataframe(ruta_incendio = path_incendios,\n",
        "                                      ruta_no_incendio = path_no_incendios,\n",
        "                                      radar= \"LST_Day\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "  \"\"\"\n",
        "  lst_archivos_incendios = []\n",
        "  lst_archivos_no_incendios = []\n",
        "  print(len(lst_archivos_incendios), len(lst_archivos_no_incendios))\n",
        "  print(ruta_incendio, ruta_no_incendio)\n",
        "  # Listado de incendios\n",
        "  lista_ficheros_incendios = []\n",
        "  n_varibles_x =reshape_x * reshape_y\n",
        "  print(\"Creando lista de incendios a leer...\")\n",
        "  for file in os.listdir(ruta_incendio):\n",
        "      if radar in file and 'jpeg' in file:\n",
        "            lst_archivos_incendios.append(file)\n",
        "\n",
        "  for file in os.listdir(ruta_no_incendio):\n",
        "      if radar in file and 'jpeg' in file:\n",
        "          lst_archivos_no_incendios.append(file)  \n",
        "  print(\"Leyendo imágenes...\")\n",
        "\n",
        "  contador = 0\n",
        "    \n",
        "  for file in  lst_archivos_incendios:\n",
        "      imagen = cv2.imread(ruta_incendio + file)[0:reshape_x, 0:reshape_y, :]\n",
        "      image_grey = np.array(cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY))\n",
        "      if \"incendio\" in file:\n",
        "        image_grey = cv2.resize(image_grey, (0,0), fx=0.01, fy=0.01) # Reduzcol 99%\n",
        "        n_varibles_x =int(reshape_x/100 * reshape_y/100)\n",
        "      image_grey_reshape = image_grey.reshape(n_varibles_x)\n",
        "      lst_grey_reshape = image_grey_reshape.tolist()\n",
        "      lst_grey_reshape.append(2) # Incorporo el target\n",
        "      lista_ficheros_incendios.append(lst_grey_reshape)\n",
        "      contador = contador + 1\n",
        "      print(\"Imágenes leídas: {}\".format(contador))\n",
        "      #print(\"Imágenes leídas: {}\".format(contador), end='\\r')\n",
        "\n",
        "  for file in  lst_archivos_no_incendios:\n",
        "      imagen = cv2.imread(ruta_no_incendio + file)[0:reshape_x, 0:reshape_y, :]\n",
        "      image_grey = np.array(cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY))\n",
        "      if \"incendio\" in file:\n",
        "        image_grey = cv2.resize(image_grey, (0,0), fx=0.01, fy=0.01) # Reduzcol 99%\n",
        "        n_varibles_x =int(reshape_x/100 * reshape_y/100)\n",
        "      image_grey_reshape = image_grey.reshape(n_varibles_x)\n",
        "      lst_grey_reshape = image_grey_reshape.tolist()\n",
        "      lst_grey_reshape.append(1) # Incorporo el target\n",
        "      lista_ficheros_incendios.append(lst_grey_reshape)\n",
        "      contador = contador + 1\n",
        "      print(\"Imágenes leídas: {}\".format(contador))\n",
        "    \n",
        "  # Lo paso a DataFrame\n",
        "  print(\"Lo paso a dataframe...\")\n",
        "  lst_features = []\n",
        "  lst_features_x = []\n",
        "  for i in range(n_varibles_x):\n",
        "      lst_features.append(\"col\" + str(i))\n",
        "      lst_features_x.append(\"col\" + str(i))\n",
        "  lst_features.append(\"target\")\n",
        "  print(len(lst_features))\n",
        "  # Genero el dataframe en sí\n",
        "  df = pd.DataFrame(data = lista_ficheros_incendios, columns =lst_features )\n",
        "\n",
        "  lista_id = []\n",
        "  for item in lst_archivos_incendios:\n",
        "    id = int(item.split(radar[0])[0])\n",
        "    lista_id.append(id)\n",
        "  for item in lst_archivos_no_incendios:\n",
        "    id = int(item.split(radar[0])[0])\n",
        "    lista_id.append(id)\n",
        "\n",
        "  print(len(lst_archivos_incendios), len(lst_archivos_no_incendios), len(lista_id),\n",
        "        len(df))\n",
        "\n",
        "  df_id = pd.DataFrame({'id': lista_id})\n",
        "  df = pd.concat([df, df_id], axis=1)\n",
        "  print(len(df))\n",
        "  # Barajo el dataframe\n",
        "  df=df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  return df, lst_features_x\n",
        "\n",
        "# DATOS DL\n",
        "# Generamos el dataframe de temperatura\n",
        "inicio = time.time()\n",
        "df_temperatura, lst_features_x = genera_dataframe(ruta_incendio = path_incendios,\n",
        "                                      ruta_no_incendio = path_no_incendios,\n",
        "                                      radar= \"LST_Day\",\n",
        "                                      reshape_x = 23,\n",
        "                                      reshape_y = 23)\n",
        "final = time.time()\n",
        "print(\"Ha tardado en generar el dataframe {} minutos\".format((final-inicio)/60))\n",
        "df_temperatura.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3eBeCZXpDSC"
      },
      "outputs": [],
      "source": [
        "# Generamos procesos previos:\n",
        "X_train, X_test, y_train, y_test = preprocesado(df_temperatura)\n",
        "\n",
        "# REGRESIÓN LOGÍSTICA\n",
        "# Hiperparámetros\n",
        "C = [0.9, 0.8, 0.7] # Intensidad de la regularización\n",
        "regularization_methods = [\"l1\", \"l2\"] # Métodos de regularización\n",
        "max_iterations = [100, 1000, 10000, 10000] # Números máximos de iteraciones\n",
        "\n",
        "temperatura_result_logistica = regresion_logistica(X_train = X_train,\n",
        "                                              X_test = X_test,\n",
        "                                              y_train = y_train,\n",
        "                                              y_test = y_test,\n",
        "                                              C = C, \n",
        "                                regularization_methods = regularization_methods,\n",
        "                                max_iterations = max_iterations)\n",
        "\n",
        "# XGBOOST\n",
        "# Hiperparámetros\n",
        "learning_rate = [0.001] # Tasa de aprendizaje\n",
        "max_depth = [10] # Profundidaz máxima\n",
        "n_estimators = [100] # Número de árboles\n",
        "temperatura_result_xgboost = func_xgboost(X_train = X_train,\n",
        "                                      X_test = X_test,\n",
        "                                      y_train = y_train,\n",
        "                                      y_test = y_test,\n",
        "                                      learning_rate = learning_rate,\n",
        "                                      max_depth = max_depth,\n",
        "                                    n_estimators = n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLhS0wl8paYe"
      },
      "outputs": [],
      "source": [
        "temp_best_result_log = max(\n",
        "    temperatura_result_logistica, key=lambda x: x['f1_score'])\n",
        "print(temp_best_result_log)\n",
        "temp_best_result_xgboost = max(\n",
        "    temperatura_result_xgboost, key=lambda x: x['f1_score'])\n",
        "print(temp_best_result_xgboost)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zg9x6poHT3ld",
        "wLmzU-cAT_pV",
        "Of6350-PUFPA",
        "CuvxNZKOXh6R"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
